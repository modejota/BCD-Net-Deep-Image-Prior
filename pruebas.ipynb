{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import sys\n",
    "import torchvision\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# import scipy.io\n",
    "# from utils.utils_metric import batch_PSNR, batch_SSIM\n",
    "\n",
    "import options \n",
    "\n",
    "from datasets.main_dataset import get_dataloaders\n",
    "from loss import loss_fn\n",
    "from networks.dnet import get_dnet\n",
    "from networks.knet import get_knet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.1\n",
      "using device: cpu\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "torch.backends.cudnn.enabled = True\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "USE_GPU = False\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda:1')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print('using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size               : 2\n",
      "patch_size               : 128\n",
      "epochs                   : 5\n",
      "pretraining_epochs       : 1\n",
      "print_freq               : 1\n",
      "save_model_freq          : 20\n",
      "lr_C                     : 0.0001\n",
      "lr_M                     : 0.0001\n",
      "gamma                    : 0.1\n",
      "clip_grad_C              : 10000.0\n",
      "clip_grad_M              : 100000.0\n",
      "train_data_path          : /data/BasesDeDatos/Camelyon/Camelyon17/training/Toy/\n",
      "pre_kernel_path          : \n",
      "log_dir                  : ./log\n",
      "model_dir                : ./model\n",
      "resume                   : \n",
      "num_workers              : 8\n",
      "sigmaRui_h_sq            : 0.001\n",
      "sigmaRui_e_sq            : 0.001\n",
      "theta                    : 0.5\n",
      "pre_kl                   : 100.0\n",
      "pre_mse                  : 0.01\n",
      "code_len                 : 30\n",
      "CNet                     : unet_6\n",
      "MNet                     : resnet_18_in\n",
      "max_size                 : 3\n",
      "dirichlet_para_stretch   : 20000\n",
      "prekernels               : \n",
      "nrow                     : 8\n",
      "epoch_start_test         : 20\n",
      "skip_grad                : 1000000.0\n",
      "warm_up_epoch            : 0\n",
      "kl_dir_weight            : 1.0\n",
      "run_mode                 : center_0\n"
     ]
    }
   ],
   "source": [
    "#args = set_opts()\n",
    "args = options.set_opts_jp()\n",
    "for arg in vars(args):\n",
    "    print('{:<25s}: {:s}'.format(arg, str(getattr(args, arg))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch, args):\n",
    "    if epoch <= 60:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = 1e-4\n",
    "    elif epoch <= 80:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = 1e-5\n",
    "    else:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnet <--> dnet\n",
    "# mnet <--> knet\n",
    "cnet = get_dnet(args.CNet)\n",
    "mnet = get_knet(args.MNet, kernel_size=3)\n",
    "cnet = cnet.to(device)\n",
    "mnet = mnet.to(device)\n",
    "optimizer_c = optim.Adam(cnet.parameters(), args.lr_C)\n",
    "optimizer_m = optim.Adam(mnet.parameters(), args.lr_M)\n",
    "pretraining_epochs = args.pretraining_epochs\n",
    "#writer.. = SummaryWriter(args.log_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available patches: 6\n",
      "Available patches: 4\n"
     ]
    }
   ],
   "source": [
    "args.epoch_start = 0\n",
    "step = 0\n",
    "#step_img = {x: 0 for x in _modes}\n",
    "os.makedirs(args.log_dir, exist_ok=True)\n",
    "os.makedirs(args.model_dir, exist_ok=True)\n",
    "\n",
    "dataloaders = get_dataloaders(args)\n",
    "train_dataloader = dataloaders['train']\n",
    "val_dataloader = dataloaders['val']\n",
    "test_dataloader = dataloaders['test']\n",
    "\n",
    "num_step_per_epoch = len(train_dataloader)\n",
    "pre_optimizer_c = optim.Adam(cnet.parameters(), lr=5e-4)\n",
    "pre_optimizer_m = optim.Adam(mnet.parameters(), lr=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretraining: Epoch: 1, Loss=-inf\n"
     ]
    }
   ],
   "source": [
    "loss_per_epoch = {'loss' : 0, 'loss_mse' : 0, 'loss_kl_h' : 0, 'loss_kl_e' : 0 }\n",
    "for epoch in range(args.pretraining_epochs):\n",
    "    tic = time.time()\n",
    "    cnet.train()\n",
    "    mnet.train()\n",
    "\n",
    "    train_loss = 0\n",
    "    for ii, data in enumerate(train_dataloader):\n",
    "        y = data[0].to(device) # shape: (batch_size, 3, H, W)\n",
    "        mR = data[1].to(device)  # shape: (batch_size, 1, 3, 2)\n",
    "        \n",
    "        pre_optimizer_m.zero_grad()\n",
    "        pre_optimizer_c.zero_grad()\n",
    "        out_MNet_mean, out_Mnet_var = mnet(y) # shape: (batch_size, 3, 2), (batch_size, 1, 2)\n",
    "        out_CNet = cnet(y) # shape: (batch_size, 2, H, W)\n",
    "\n",
    "        loss, loss_mse, loss_kl, _, _ = loss_fn(  out_CNet, out_MNet_mean, out_Mnet_var, y, mR,\n",
    "                                            args.sigmaRui_h_sq, args.sigmaRui_e_sq, \n",
    "                                            pretraining=True, pre_mse = args.pre_mse, pre_kl = args.pre_kl\n",
    "                                            )\n",
    "\n",
    "        loss.backward()\n",
    "        pre_optimizer_m.step()\n",
    "        pre_optimizer_c.step()\n",
    "        train_loss += loss.item() / num_step_per_epoch\n",
    "    \n",
    "    print(f\"Pretraining: Epoch: {epoch + 1}, Loss={train_loss:.4e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0001/0005], Iter 00001/00003, loss=-4.5099e+02, loss_mse=5.6816e+03, loss_kl_h=-1.0563e+00, loss_kl_e=-6.5826e+03, grad_norm_C=1.00e+04/3.79e+05, grad_norm_M=1.00e+05/2.04e+06lr_C=1.0e-04, lr_M=1.0e-04\n",
      "[Epoch 0001/0005], Iter 00002/00003, loss=-inf, loss_mse=1.0440e+03, loss_kl_h=-inf, loss_kl_e=-6.7920e+03, grad_norm_C=1.00e+04/2.20e+04, grad_norm_M=1.00e+05/1.64e+04lr_C=1.0e-04, lr_M=1.0e-04\n",
      "[Epoch 0001/0005], Iter 00003/00003, loss=-inf, loss_mse=3.6944e+02, loss_kl_h=-inf, loss_kl_e=-6.5659e+03, grad_norm_C=1.00e+04/1.18e+04, grad_norm_M=1.00e+05/1.64e+04lr_C=1.0e-04, lr_M=1.0e-04\n",
      "[Epoch 0001/0005], Iter 00003/00003, loss=-inf, loss_mse=2.3650e+03, loss_kl_h=-inf, loss_kl_e=-6.6468e+03, \n",
      "This epoch took time 23.46 s.\n",
      "[Epoch 0002/0005], Iter 00001/00003, loss=-inf, loss_mse=1.0729e+03, loss_kl_h=-inf, loss_kl_e=-6.7697e+03, grad_norm_C=1.00e+04/1.49e+04, grad_norm_M=1.00e+05/1.67e+04lr_C=1.0e-04, lr_M=1.0e-04\n",
      "[Epoch 0002/0005], Iter 00002/00003, loss=-inf, loss_mse=2.1266e+02, loss_kl_h=-inf, loss_kl_e=-7.0056e+03, grad_norm_C=1.00e+04/3.50e+03, grad_norm_M=1.00e+05/1.69e+04lr_C=1.0e-04, lr_M=1.0e-04\n",
      "[Epoch 0002/0005], Iter 00003/00003, loss=-inf, loss_mse=6.1595e+02, loss_kl_h=-inf, loss_kl_e=-7.0961e+03, grad_norm_C=1.00e+04/8.16e+03, grad_norm_M=1.00e+05/1.71e+04lr_C=1.0e-04, lr_M=1.0e-04\n",
      "[Epoch 0002/0005], Iter 00003/00003, loss=-inf, loss_mse=2.9989e+03, loss_kl_h=-inf, loss_kl_e=-1.3604e+04, \n",
      "This epoch took time 35.28 s.\n",
      "[Epoch 0003/0005], Iter 00001/00003, loss=-inf, loss_mse=3.5891e+02, loss_kl_h=-inf, loss_kl_e=-7.1843e+03, grad_norm_C=1.00e+04/1.27e+04, grad_norm_M=1.00e+05/1.72e+04lr_C=1.0e-04, lr_M=1.0e-04\n",
      "[Epoch 0003/0005], Iter 00002/00003, loss=-inf, loss_mse=6.1599e+02, loss_kl_h=-inf, loss_kl_e=-7.2970e+03, grad_norm_C=1.00e+04/9.06e+03, grad_norm_M=1.00e+05/1.74e+04lr_C=1.0e-04, lr_M=1.0e-04\n",
      "[Epoch 0003/0005], Iter 00003/00003, loss=-inf, loss_mse=4.0426e+02, loss_kl_h=-inf, loss_kl_e=-7.5663e+03, grad_norm_C=1.00e+04/1.85e+03, grad_norm_M=1.00e+05/1.76e+04lr_C=1.0e-04, lr_M=1.0e-04\n",
      "[Epoch 0003/0005], Iter 00003/00003, loss=-inf, loss_mse=3.4586e+03, loss_kl_h=-inf, loss_kl_e=-2.0953e+04, \n",
      "This epoch took time 47.15 s.\n",
      "[Epoch 0004/0005], Iter 00001/00003, loss=-inf, loss_mse=3.4724e+02, loss_kl_h=-inf, loss_kl_e=-7.7202e+03, grad_norm_C=1.00e+04/1.28e+04, grad_norm_M=1.00e+05/1.79e+04lr_C=1.0e-04, lr_M=1.0e-04\n",
      "[Epoch 0004/0005], Iter 00002/00003, loss=-inf, loss_mse=4.7751e+02, loss_kl_h=-inf, loss_kl_e=-7.6857e+03, grad_norm_C=1.00e+04/5.77e+03, grad_norm_M=1.00e+05/1.79e+04lr_C=1.0e-04, lr_M=1.0e-04\n",
      "[Epoch 0004/0005], Iter 00003/00003, loss=-inf, loss_mse=4.5062e+02, loss_kl_h=-inf, loss_kl_e=-7.9349e+03, grad_norm_C=1.00e+04/6.53e+03, grad_norm_M=1.00e+05/1.80e+04lr_C=1.0e-04, lr_M=1.0e-04\n",
      "[Epoch 0004/0005], Iter 00003/00003, loss=-inf, loss_mse=3.8837e+03, loss_kl_h=-inf, loss_kl_e=-2.8733e+04, \n",
      "This epoch took time 58.77 s.\n",
      "[Epoch 0005/0005], Iter 00001/00003, loss=-inf, loss_mse=1.2276e+02, loss_kl_h=-inf, loss_kl_e=-8.1310e+03, grad_norm_C=1.00e+04/1.53e+03, grad_norm_M=1.00e+05/1.82e+04lr_C=1.0e-04, lr_M=1.0e-04\n",
      "[Epoch 0005/0005], Iter 00002/00003, loss=-inf, loss_mse=6.0856e+02, loss_kl_h=-inf, loss_kl_e=-8.4274e+03, grad_norm_C=1.00e+04/1.12e+04, grad_norm_M=1.00e+05/1.89e+04lr_C=1.0e-04, lr_M=1.0e-04\n",
      "[Epoch 0005/0005], Iter 00003/00003, loss=-inf, loss_mse=1.0693e+02, loss_kl_h=-inf, loss_kl_e=-8.5649e+03, grad_norm_C=1.00e+04/1.78e+03, grad_norm_M=1.00e+05/1.87e+04lr_C=1.0e-04, lr_M=1.0e-04\n",
      "[Epoch 0005/0005], Iter 00003/00003, loss=-inf, loss_mse=4.1631e+03, loss_kl_h=-inf, loss_kl_e=-3.7108e+04, \n",
      "This epoch took time 68.92 s.\n",
      "Reached the maximal epochs! Finish training\n"
     ]
    }
   ],
   "source": [
    "loss_per_epoch = {'loss' : 0, 'loss_mse' : 0, 'loss_kl_h' : 0, 'loss_kl_e' : 0 }\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "\n",
    "    adjust_learning_rate(optimizer_m, epoch, args)\n",
    "    adjust_learning_rate(optimizer_c, epoch, args)\n",
    "    grad_norm_C = grad_norm_M = 0\n",
    "    lr_C = optimizer_c.param_groups[0]['lr']\n",
    "    lr_M = optimizer_m.param_groups[0]['lr']\n",
    "    loss_mse_per_epoch = 0\n",
    "    for ii, data in enumerate(train_dataloader):\n",
    "\n",
    "        y = data[0].to(device) # shape: (batch_size, 3, H, W)\n",
    "        mR = data[1].to(device)  # shape: (batch_size, 1, 3, 2)\n",
    "\n",
    "        optimizer_m.zero_grad()\n",
    "        optimizer_c.zero_grad()\n",
    "\n",
    "        out_MNet_mean, out_Mnet_var = mnet(y) # shape: (batch_size, 3, 2), (batch_size, 1, 2)\n",
    "        out_CNet = cnet(y) # shape: (batch_size, 2, H, W)\n",
    "\n",
    "        loss, loss_mse, loss_kl, loss_kl_h, loss_kl_e = loss_fn(  out_CNet, out_MNet_mean, out_Mnet_var, y, mR,\n",
    "                                            args.sigmaRui_h_sq, args.sigmaRui_e_sq\n",
    "                                            )\n",
    "        \n",
    "        loss.backward()\n",
    "        total_norm_C = nn.utils.clip_grad_norm_(cnet.parameters(), args.clip_grad_C)\n",
    "        total_norm_M = nn.utils.clip_grad_norm_(mnet.parameters(), args.clip_grad_M)\n",
    "        grad_norm_C = grad_norm_C + total_norm_C / num_step_per_epoch\n",
    "        grad_norm_M = grad_norm_M + total_norm_M / num_step_per_epoch\n",
    "        optimizer_c.step()\n",
    "        optimizer_m.step()\n",
    "\n",
    "        loss_per_epoch['loss'] += loss.item() / num_step_per_epoch\n",
    "        loss_per_epoch['loss_mse'] += loss_mse.item() / num_step_per_epoch\n",
    "        loss_per_epoch['loss_kl_h'] += loss_kl_h.item() / num_step_per_epoch\n",
    "        loss_per_epoch['loss_kl_e'] += loss_kl_e.item() / num_step_per_epoch\n",
    "\n",
    "        r_con_e = torch.clamp(out_CNet[:, :1, :, : ].detach().data, 0.0, 1.0)\n",
    "        r_con_h = torch.clamp(out_CNet[:, 1:, :, :].detach().data, 0.0, 1.0)\n",
    "\n",
    "\n",
    "        # mse = F.mse_loss(y[0,:,:], out_CNet[0,0,:,:])\n",
    "        loss_mse_per_epoch += loss_mse / num_step_per_epoch\n",
    "\n",
    "        if (ii + 1) % args.print_freq == 0:\n",
    "            print(f\"[Epoch {epoch + 1:0>4d}/{args.epochs:0>4d}], Iter {ii + 1:0>5d}/{num_step_per_epoch:0>5d}, \"\n",
    "                  f\"loss={loss.item():.4e}, loss_mse={loss_mse.item():.4e}, loss_kl_h={loss_kl_h.item():.4e}, loss_kl_e={loss_kl_e.item():.4e}, \"\n",
    "                  f\"grad_norm_C={args.clip_grad_C:.2e}/{total_norm_C:.2e}, grad_norm_M={args.clip_grad_M:.2e}/{total_norm_M:.2e}\"\n",
    "                  f\"lr_C={lr_C:.1e}, lr_M={lr_M:.1e}\")\n",
    "\n",
    "            step += 1\n",
    "\n",
    "    print(f\"[Epoch {epoch + 1:0>4d}/{args.epochs:0>4d}], Iter {ii + 1:0>5d}/{num_step_per_epoch:0>5d}, \"\n",
    "          f\"loss={loss_per_epoch['loss']:.4e}, loss_mse={loss_per_epoch['loss_mse']:.4e}, loss_kl_h={loss_per_epoch['loss_kl_h']:.4e}, loss_kl_e={loss_per_epoch['loss_kl_e']:.4e}, \"\n",
    "          )\n",
    "\n",
    "    toc = time.time()\n",
    "    print(f'This epoch took time {toc - tic:.2f} s.')\n",
    "#writer.close()\n",
    "print('Reached the maximal epochs! Finish training')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('DVBCD')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c6ad41a6645221cf1bf85d33afd32a0947fad2555ebbd1ea9580495d050a48a0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
